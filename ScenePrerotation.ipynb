{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9d3776f8-fe0a-42d6-97d9-7b4875ff32e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from tqdm import tqdm\n",
    "from solver import Up2P\n",
    "from utils.rotation_utils import get_upward_with_dev, get_rt_mtx, validate_sol\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eebe7650-4355-4a89-96e7-778e2f18d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    max_depth: float = 10.\n",
    "    img_width: int = 640\n",
    "    img_height: int = 640\n",
    "    focal_length: int = 3 * (img_width * 0.5) / np.tan(60.0 * np.pi / 180.0);\n",
    "    min_depth: float = 1.\n",
    "    max_depth: float = 1.1\n",
    "    inliers_ratio: float = 1.\n",
    "    outlier_dist: float = 30.\n",
    "    \n",
    "    # [TODO][IMPORTNAT]: not properly tested, be aware of using for\n",
    "    # some experiments\n",
    "    pixel_noise: float = 0.\n",
    "    \n",
    "conf = Config()\n",
    "dtype = torch.float64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d96120e-9a52-4783-835b-941482f7538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_correspondence(x: torch.tensor, conf: Config):\n",
    "    x = to_camera_coords(x, conf)\n",
    "    x *= random.uniform(conf.min_depth, conf.max_depth)\n",
    "    \n",
    "    assert x.shape == (3,)    \n",
    "    return x\n",
    "\n",
    "def transform_correspondence(X: torch.tensor, R: torch.tensor, t: torch.tensor):\n",
    "    return R @ X + t\n",
    "\n",
    "def to_homogeneous(x):\n",
    "    return torch.cat([x, torch.ones(1)])\n",
    "\n",
    "def to_camera_coords(x: torch.tensor, conf: Config = conf):\n",
    "    x = to_homogeneous(x)\n",
    "    \n",
    "    x[0] -= conf.img_width // 2\n",
    "    x[1] -= conf.img_height // 2\n",
    "    x[:2] /= conf.focal_length\n",
    "    x /= x.norm()\n",
    "    \n",
    "    return x\n",
    "\n",
    "def reproject(X, R, t, conf: Config = conf):\n",
    "    translated = R.T @ (X - t)\n",
    "    translated[:2] /= translated[2]\n",
    "    translated[:2] *= conf.focal_length\n",
    "    translated[0] += conf.img_width // 2\n",
    "    translated[1] += conf.img_height // 2\n",
    "    \n",
    "    return translated[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "650db9a4-72d0-4d6e-a13b-1068502ba7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.rotation_utils import get_random_upward\n",
    "\n",
    "def get_random_image_point(conf: Config):\n",
    "    x = random.uniform(0, conf.img_width)\n",
    "    y = random.uniform(0, conf.img_height)\n",
    "    x = torch.tensor([x, y], dtype=torch.float64)    \n",
    "    return x\n",
    "\n",
    "\n",
    "def generate_example(R, t, conf: Config = conf):\n",
    "    x1, x2 = get_random_image_point(conf), get_random_image_point(conf)\n",
    "    X1, X2 = generate_correspondence(x1.clone(), conf),\\\n",
    "             generate_correspondence(x2.clone(), conf)\n",
    "    X1, X2 = transform_correspondence(X1, R, t), transform_correspondence(X2, R, t)\n",
    "    \n",
    "    # [TODO][IMPORTNAT]: not properly tested, be aware of using for\n",
    "    # some experiments\n",
    "    if conf.pixel_noise != 0:\n",
    "        x1noise = np.random.normal(0, conf.pixel_noise, 2)\n",
    "        x2noise = np.random.normal(0, conf.pixel_noise, 2)\n",
    "        \n",
    "        if torch.all(x1[0] + x1noise > 0) and torch.all(x1[1] + x1noise < conf.img_width):\n",
    "            x1 += x1noise\n",
    "            assert x1[0] > 0 and x1[1] < conf.img_width, f\"{x1}\"\n",
    "            \n",
    "        if torch.all(x2[0] + x2noise > 0) and torch.all(x2[1] + x2noise < conf.img_height):\n",
    "            x2 += x2noise\n",
    "            assert x2[0] > 0 and x2[1] < conf.img_height, f\"{x2}\"\n",
    "        \n",
    "        assert x1[0] > 0 and x1[1] < conf.img_width, f\"{x1}\"\n",
    "        assert x2[0] > 0 and x2[1] < conf.img_height, f\"{x2}\"\n",
    "        \n",
    "    return x1, x2, X1, X2 \n",
    "        \n",
    "def generate_examples(num_of_examples: int,\n",
    "                      dev: Tuple[float, float] = (0., 0.), conf: Config = conf):\n",
    "    num_of_examples = num_of_examples // 2\n",
    "    \n",
    "    num_inliers = num_of_examples * conf.inliers_ratio\n",
    "    num_outliers = num_of_examples - num_inliers\n",
    "    \n",
    "    if num_of_examples == 0:\n",
    "        num_of_examples, num_inliers, num_outliers = 1, 1, 0\n",
    "    \n",
    "    R, rand_angle = get_random_upward(*dev)\n",
    "    t = torch.rand(3, )\n",
    "        \n",
    "    # [TODO] [IMPORTANT]: under such generation we cannot get model where one of the points is an inlier\n",
    "    xs, Xs, inliers = [], [], []\n",
    "    for i in range(num_of_examples):\n",
    "        x1, x2, X1, X2 = generate_example(R, t)\n",
    "        Xs.append((X1, X2))\n",
    "        \n",
    "        if i < num_inliers:\n",
    "            xs.append((x1, x2))\n",
    "            inliers.append(True)\n",
    "        else:\n",
    "            xs.append((generate_outlier(x1, conf), generate_outlier(x2, conf)))\n",
    "            inliers.append(False)\n",
    "            \n",
    "    return xs, Xs, inliers, R, t, rand_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2230ac5f-cb94-4a5c-bb15-8545a1f0559c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 538.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'res=672, skipped=324, Success rate: 0.9940828402366864'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREROTATE = True\n",
    "res = 0\n",
    "skipped = 0\n",
    "\n",
    "for _ in tqdm(range(1000)):\n",
    "    solver = Up2P()\n",
    "\n",
    "    x=0\n",
    "    z=40\n",
    "    prerot = get_rt_mtx(roll=x, pitch=z, yaw=0, device=device, dtype=torch.float64)\n",
    "\n",
    "    xs, Xs, _, Rg, tg, rand_angle = generate_examples(2, dev=(x, z))\n",
    "    xs, Xs = list(xs[0]), list(Xs[0])\n",
    "    txs, tXs = xs[1][0], Xs[1][0]\n",
    "\n",
    "    xsc = torch.zeros((2, 3))\n",
    "    xsc[0] = to_camera_coords(xs[0], conf)\n",
    "    xsc[1] = to_camera_coords(xs[1], conf)\n",
    "\n",
    "    if PREROTATE:\n",
    "        Xs = [prerot.T @ X for X in Xs]\n",
    "\n",
    "    err, Re, te = None, None, None\n",
    "    for R, t in solver(xsc, torch.stack(Xs)):    \n",
    "        rp = reproject(tXs, R, t, conf)\n",
    "        cerr = (txs - rp).norm()\n",
    "        \n",
    "        if err is None or cerr < err:\n",
    "            err = cerr\n",
    "            Re, te = R, t\n",
    "\n",
    "    if Re is None:\n",
    "        skipped += 1\n",
    "    else:\n",
    "        res += torch.allclose(reproject(Xs[0], Re, te, conf), xs[0]) and torch.allclose(reproject(Xs[1], Re, te, conf), xs[1])\n",
    "        \n",
    "f\"{res=}, {skipped=}, Success rate: {res / (1000 - skipped)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f7a0b366-d7a3-4e85-9bf7-6a0a2101814c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 537.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'res=995, skipped=0, Success rate: 0.995'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = 0\n",
    "skipped = 0\n",
    "\n",
    "for _ in tqdm(range(1000)):\n",
    "\n",
    "    solver = Up2P()\n",
    "\n",
    "    xs, Xs, _, Rg, tg, rand_angle = generate_examples(2, dev=(0, 0))\n",
    "    xs, Xs = list(xs[0]), list(Xs[0])\n",
    "    txs, tXs = xs[1][0], Xs[1][0]\n",
    "\n",
    "    xsc = torch.zeros((2, 3))\n",
    "    xsc[0] = to_camera_coords(xs[0], conf)\n",
    "    xsc[1] = to_camera_coords(xs[1], conf)\n",
    "\n",
    "    err, Re, te = None, None, None\n",
    "    for R, t in solver(xsc, torch.stack(Xs)):    \n",
    "        rp = reproject(tXs, R, t, conf)\n",
    "        cerr = (txs - rp).norm()\n",
    "        \n",
    "        if err is None or cerr < err:\n",
    "            err = cerr\n",
    "            Re, te = R, t\n",
    "\n",
    "    if Re is None:\n",
    "        skipped += 1\n",
    "    else:\n",
    "        res += torch.allclose(reproject(Xs[0], Re, te, conf), xs[0]) and torch.allclose(reproject(Xs[1], Re, te, conf), xs[1])\n",
    "    \n",
    "f\"{res=}, {skipped=}, Success rate: {res / (1000 - skipped)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4296e496-7c80-4041-8540-3dc9019989d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfcc6ef-fa6d-4dc9-9360-e6010bc6750b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
