{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa686a70-2c4a-45c1-95c2-6a051089700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c667c9b-b0de-48ba-899e-11c197ca6f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import statistics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(16,10)})\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.transform import Rotation\n",
    "from pytorch3d.transforms import matrix_to_quaternion\n",
    "\n",
    "import poselib\n",
    "\n",
    "from solver import Up2P, Solver\n",
    "from SolverPipeline import P3PBindingWrapperPipeline\n",
    "from SolverPipeline import SolverPipeline as SP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6977b51-686f-4339-be7e-1b5d98141fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "VSP = \"dataset/StMarysChurch_matches\"\n",
    "seq = [3, 5, 13]\n",
    "SHOW_SINGLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86247417-9a9f-4d8d-bc28-c9ea829a5b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    ransac_thresh = 13.\n",
    "    max_side_length = 320\n",
    "    max_ransac_iters = 10000\n",
    "    \n",
    "conf = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64b98253-1a4f-4d7e-83da-ef91061380ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3pwrapper = P3PBindingWrapperPipeline(\n",
    "    ransac_conf = {\n",
    "       # 'max_reproj_error': args.ransac_thresh\n",
    "       'min_iterations': min(100, conf.max_ransac_iters),\n",
    "       'max_iterations': conf.max_ransac_iters,\n",
    "       'progressive_sampling': True,\n",
    "       'max_prosac_iterations': conf.max_ransac_iters\n",
    "    },\n",
    "    \n",
    "    bundle_adj_conf = {\n",
    "        'loss_scale' : 1.0,\n",
    "    }                                              \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b65ffd7-72cc-4f87-b7f3-02f4cbd5da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = {'model': 'SIMPLE_RADIAL', 'width': 320, 'height': 180, 'params': [277.4716064453125, 160.0, 90.0, 0.0]}\n",
    "pts2D = [\n",
    "    np.array([192.12533569,  19.14378548]),\n",
    "    np.array([91.60398102, 26.73556519]),\n",
    "    np.array([180.32232666,  33.99654388]),\n",
    "    np.array([192.33743286,  37.74715424]),\n",
    "    np.array([188.43441772,  41.1788559 ])\n",
    "]\n",
    "\n",
    "pts3D = [\n",
    "    np.array([ 11.86180782, -14.56327057,  -0.92378181]),\n",
    "    np.array([ 6.79015875, -9.56949902, -1.78533459]),\n",
    "    np.array([11.95058823, -0.89410073, -0.36948705]), \n",
    "    np.array([ 12.17275715, -13.31939125,  -0.34633577]),\n",
    "    np.array([ 7.56372643, -2.60536647, -2.24980545])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab102379-7cf2-48d3-91fb-a57453e99ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    \n",
    "    def __call__(self, pts: np.array, sample_size: int):\n",
    "        n = len(pts)\n",
    "        assert n > sample_size\n",
    "        \n",
    "        idcs = np.random.choice(n, sample_size)\n",
    "        \n",
    "        return pts[idcs], idcs\n",
    "    \n",
    "sampler = Sampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c90de2f1-b8f2-4778-9b11-bfda372fc98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "\n",
    "class Camera:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 w: int,\n",
    "                 h: int,\n",
    "                 f: float,\n",
    "                 cc: Tuple[int, int]\n",
    "                ) -> None:\n",
    "        self.f = f\n",
    "        self.cx, self.cy = cc\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        \n",
    "    def pix2cam(self, x: np.ndarray):\n",
    "        assert x.ndim == 2\n",
    "        \n",
    "        x = np.concatenate([x, np.ones(x.shape[0])[:, np.newaxis]], axis=1)\n",
    "        x[:, 0] -= self.cx\n",
    "        x[:, 1] -= self.cy\n",
    "        x[:, :2] = x[:, :2] / self.f\n",
    "        x /= np.linalg.norm(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def cam2pix(self, x: np.ndarray):\n",
    "        assert x.ndim == 2\n",
    "            \n",
    "        x[:, :2] /= x[:, 2]\n",
    "        x[:, :2] = x[:, :2] * self.f\n",
    "        x[:, 0] += self.cx\n",
    "        x[:, 1] += self.cy\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_camera_dict(camera: Dict):\n",
    "        w, h = camera[\"width\"], camera[\"height\"]\n",
    "        params = camera[\"params\"]\n",
    "        f, cx, cy, _ = params\n",
    "        \n",
    "        return Camera(w, h, f, (cx, cy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "07a587ea-16bb-4f51-afd9-4c95d977c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplacementRefinerSolver(Solver):\n",
    "    \n",
    "    def __init__(self, min_sample_size: int = 100, models_to_evaluate: int = 3, verbose: bool = False):\n",
    "        self.min_sample_size = min_sample_size\n",
    "        self.models_to_evaluate = models_to_evaluate\n",
    "        self.internal_solver = Up2P()\n",
    "        self.verbose = verbose\n",
    "        self.camera: Camera = None\n",
    "    \n",
    "    def get_sample_size(self) -> int:\n",
    "        return self.min_sample_size\n",
    "    \n",
    "    def __call__(self, x, X, camera_dict):\n",
    "        # assert x.shape == (self.min_sample_size, 3)\n",
    "        # assert X.shape == (self.min_sample_size, 3)\n",
    "        assert x.shape[0] == X.shape[0]\n",
    "        \n",
    "        self.camera = Camera.from_camera_dict(camera_dict)\n",
    "        x = self.camera.pix2cam(x)\n",
    "        \n",
    "        idcs = np.random.choice(len(X), self.internal_solver.get_sample_size() + 1)\n",
    "        xx, XX = x[idcs[:idcs.shape[0] - 1]], X[idcs[:idcs.shape[0] - 1]]\n",
    "            \n",
    "        solv_res = self.internal_solver(xx, XX)\n",
    "            \n",
    "        err, Rf, tf = None, None, None\n",
    "        for sol in solv_res:\n",
    "            R, t = sol\n",
    "            R, t = R.detach().cpu().numpy(), t.detach().cpu().numpy()\n",
    "         \n",
    "            gts, projs = [], []\n",
    "            for (xx, XX) in zip(x, X):\n",
    "                proj = R.T @ (XX - t)\n",
    "                proj = self.camera.cam2pix(proj[None, :])[0]\n",
    "                \n",
    "                gts.append(xx)\n",
    "                projs.append(proj)\n",
    "                    \n",
    "            _, angles = self._get_rot_angles(np.stack(gts)[:, 2], np.stack(projs)[:, :2])\n",
    "                \n",
    "            deg_angles = [angle.as_euler(\"XYZ\", degrees=True)[2] for angle in angles]\n",
    "\n",
    "            if self.verbose:\n",
    "                plt.hist(deg_angles, density=True, color='black', bins=np.arange(-180, 180, 5))\n",
    "                plt.xticks(range(-180, 180, 5))\n",
    "                plt.show()\n",
    "                \n",
    "            counts = np.bincount([angle + 180.0 for angle in deg_angles])\n",
    "            prerotate_with = angles[np.argmax(counts)].as_matrix()\n",
    "                \n",
    "            Xs = np.array([prerotate_with.T @ XX for XX in X.copy()])\n",
    "            iidcs = np.random.choice(len(X), self.internal_solver.get_sample_size() + 1)\n",
    "            internal_x, internal_X = x[iidcs], Xs[iidcs]\n",
    "                \n",
    "            inner_solv_res = self.internal_solver(\n",
    "                internal_x[:internal_x.shape[0] - 1],\n",
    "                internal_X[:internal_X.shape[0] - 1],\n",
    "            )\n",
    "                \n",
    "            ierr, IR, It = None, None, None\n",
    "            for iR, it in inner_solv_res:\n",
    "                rp = R.T @ (internal_X[internal_X.shape[0] - 1] - t)\n",
    "                rp = self.camera.cam2pix(rp[None, :])[0]\n",
    "                    \n",
    "                cerr = np.linalg.norm(internal_x[internal_x.shape[0] - 1] - rp)\n",
    "                if ierr is None or cerr < ierr:\n",
    "                    ierr = cerr\n",
    "                    IR, It = iR, it\n",
    "                   \n",
    "            R = prerotate_with @ IR.detach().cpu().numpy()\n",
    "            \n",
    "            rp = R.T @ (XX[idcs.shape[0] - 1] - t)\n",
    "            rp[:2] /= rp[2]\n",
    "            rp[:2] *= f\n",
    "            rp[0] += cx\n",
    "            rp[1] += cy\n",
    "                        \n",
    "            cerr = np.linalg.norm(xx[idcs.shape[0] - 1] - rp)\n",
    "            if err is None or cerr < err:\n",
    "                err = cerr\n",
    "                Rf, tf = R, t\n",
    "                \n",
    "        pose = poselib.CameraPose()\n",
    "        try:\n",
    "            pose.q = matrix_to_quaternion(torch.tensor(Rf))\n",
    "            pose.t = tf\n",
    "\n",
    "            return pose\n",
    "        except:\n",
    "            return None\n",
    "                \n",
    "                    \n",
    "    def _get_rot_angles(self, gt: np.array, proj: np.array):\n",
    "        centers = []\n",
    "        indexes = []\n",
    "        angles = []\n",
    "        for _ in range(1000):\n",
    "            idcs = np.random.choice(len(gt), 2)\n",
    "            indexes.append(idcs)\n",
    "\n",
    "            gt1, proj1 = gt[idcs[0]], proj[idcs[0]]\n",
    "            gt2, proj2 = gt[idcs[1]], proj[idcs[1]]\n",
    "\n",
    "            c = self._get_center_of_rotation(gt1, proj1, gt2, proj2)\n",
    "            centers.append(c)\n",
    "            \n",
    "        mean_c = np.array([np.median([elm[0] for elm in centers]), np.median([elm[1] for elm in centers])])\n",
    "\n",
    "        for i in range(1000):\n",
    "            idcs = indexes[i]\n",
    "\n",
    "            gt1, proj1 = gt[idcs[0]], proj[idcs[0]]\n",
    "            gt2, proj2 = gt[idcs[1]], proj[idcs[1]]\n",
    "\n",
    "            try:\n",
    "                angle = self._get_rotation(gt1, proj1, gt2, proj2, mean_c)\n",
    "            except Exception as ex:\n",
    "                continue\n",
    "\n",
    "            angles.append(angle)\n",
    "\n",
    "        return centers, angles            \n",
    "            \n",
    "    \n",
    "    def _get_intersect(self, a1, a2, b1, b2):\n",
    "        \"\"\" \n",
    "        Returns the point of intersection of the lines passing through a2,a1 and b2,b1.\n",
    "        a1: [x, y] a point on the first line\n",
    "        a2: [x, y] another point on the first line\n",
    "        b1: [x, y] a point on the second line\n",
    "        b2: [x, y] another point on the second line\n",
    "        \"\"\"\n",
    "        s = np.vstack([a1,a2,b1,b2])        # s for stacked\n",
    "        h = np.hstack((s, np.ones((4, 1)))) # h for homogeneous\n",
    "        l1 = np.cross(h[0], h[1])           # get first line\n",
    "        l2 = np.cross(h[2], h[3])           # get second line\n",
    "        x, y, z = np.cross(l1, l2)          # point of intersection\n",
    "        if z == 0:                          # lines are parallel\n",
    "            return (float('inf'), float('inf'))\n",
    "        return (x/z, y/z)\n",
    "    \n",
    "    def _get_norm_of_disp(self, gt, proj):\n",
    "        vec = (proj - gt)\n",
    "        return (-vec[1], vec[0])\n",
    "    \n",
    "    def _get_center_of_rotation(self, gt1, proj1, gt2, proj2):\n",
    "        n1, n2 = self._get_norm_of_disp(gt1, proj1), self._get_norm_of_disp(gt2, proj2)\n",
    "\n",
    "        first_center = (gt1 + proj1) / 2\n",
    "        second_center = (gt2 + proj2) / 2\n",
    "\n",
    "        c = self._get_intersect(\n",
    "            first_center,\n",
    "            first_center + n1,\n",
    "            second_center,\n",
    "            second_center + n2,\n",
    "        )\n",
    "\n",
    "\n",
    "        return c\n",
    "    \n",
    "    def _get_rotation(self, gt1, proj1, gt2, proj2, c):\n",
    "        cgt1 = gt1 - c\n",
    "        cproj1 = proj1 - c\n",
    "\n",
    "        cgt2 = gt2 - c\n",
    "        cproj2 = proj2 - c\n",
    "\n",
    "\n",
    "        res = Rotation.align_vectors(\n",
    "            a=np.array(\n",
    "                [[*cgt1, 0],\n",
    "                 [*cgt2, 0]]\n",
    "            ),\n",
    "            b=np.array(\n",
    "                [\n",
    "                    [*cproj1, 0],\n",
    "                    [*cproj2, 0]\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return res[0]\n",
    "\n",
    "ref = DisplacementRefinerSolver(verbose=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "88dbfd0b-76d2-47dd-a9bf-bbb78985b747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99 [00:00<?, ?it/s]<ipython-input-54-1fc64e9d5bd8>:170: UserWarning: Optimal rotation is not uniquely or poorly defined for the given sets of vectors.\n",
      "  res = Rotation.align_vectors(\n",
      "  0%|          | 0/99 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'multiply' did not contain a loop with signature matching types (dtype('float64'), dtype('<U22')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpy\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m pe, oe \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mp\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mf\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcamera_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(pe, oe)\n\u001b[1;32m      9\u001b[0m pose_errors\u001b[38;5;241m.\u001b[39mappend(pe)\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m(executor, path, conf, camera_dict, gts)\u001b[0m\n\u001b[1;32m      9\u001b[0m gt \u001b[38;5;241m=\u001b[39m gts[pp]\n\u001b[1;32m     10\u001b[0m c, r \u001b[38;5;241m=\u001b[39m gt[:\u001b[38;5;241m3\u001b[39m], gt[\u001b[38;5;241m3\u001b[39m:]\n\u001b[0;32m---> 12\u001b[0m pose \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpts2D\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpts3D\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcamera_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m pose\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m pose\u001b[38;5;241m.\u001b[39mR\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m pose\u001b[38;5;241m.\u001b[39mt          \n\u001b[1;32m     16\u001b[0m gt_pose \u001b[38;5;241m=\u001b[39m poselib\u001b[38;5;241m.\u001b[39mCameraPose()\n",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36mDisplacementRefinerSolver.__call__\u001b[0;34m(self, x, X, camera_dict)\u001b[0m\n\u001b[1;32m     72\u001b[0m rp \u001b[38;5;241m=\u001b[39m R\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m (XX[idcs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m t)\n\u001b[1;32m     73\u001b[0m rp[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m rp[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m---> 74\u001b[0m rp[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m f\n\u001b[1;32m     75\u001b[0m rp[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cx\n\u001b[1;32m     76\u001b[0m rp[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cy\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'multiply' did not contain a loop with signature matching types (dtype('float64'), dtype('<U22')) -> None"
     ]
    }
   ],
   "source": [
    "orientation_errors, pose_errors = [], []\n",
    "for s in seq:\n",
    "    p = f\"{VSP}/seq{s}\"\n",
    "    for f in tqdm(os.listdir(p)):\n",
    "        if f.split(\".\")[1] != \"npy\":\n",
    "            continue\n",
    "        pe, oe = process_file(ref, f\"{p}/{f}\", conf, camera_dict, gt_dict)\n",
    "        print(pe, oe)\n",
    "        pose_errors.append(pe)\n",
    "        orientation_errors.append(oe)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a51e7e6b-1286-4999-99b1-1de520e95aba",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'width'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpts2D\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpts3D\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcamera\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m res, \u001b[38;5;28mtype\u001b[39m(res)\n",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36mDisplacementRefinerSolver.__call__\u001b[0;34m(self, x, X, camera_dict)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, X, camera_dict):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# assert x.shape == (self.min_sample_size, 3)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# assert X.shape == (self.min_sample_size, 3)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera \u001b[38;5;241m=\u001b[39m \u001b[43mCamera\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_camera_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcamera_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera\u001b[38;5;241m.\u001b[39mpix2cam(x)\n\u001b[1;32m     21\u001b[0m     idcs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlen\u001b[39m(X), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minternal_solver\u001b[38;5;241m.\u001b[39mget_sample_size() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mCamera.from_camera_dict\u001b[0;34m(camera)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_camera_dict\u001b[39m(camera: Dict):\n\u001b[0;32m---> 35\u001b[0m     w, h \u001b[38;5;241m=\u001b[39m \u001b[43mcamera_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwidth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, camera_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     36\u001b[0m     params \u001b[38;5;241m=\u001b[39m camera_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     37\u001b[0m     f, cx, cy, _ \u001b[38;5;241m=\u001b[39m params\n",
      "\u001b[0;31mKeyError\u001b[0m: 'width'"
     ]
    }
   ],
   "source": [
    "res = ref(\n",
    "    np.array(pts2D),\n",
    "    np.array(pts3D),\n",
    "    camera\n",
    ")\n",
    "res, type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fa0aadf-c029-4928-8038-f931b2198c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UP2PSolverPipeline(SP):\n",
    "    \n",
    "    def __init__(self, num_models_to_eval: int = 100, verbose: bool = False):\n",
    "        self.solver = Up2P()\n",
    "        self.sampler = Sampler()\n",
    "        self.num_models_to_eval = num_models_to_eval\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __call__(self, pts2D, pts3D, camera_dict):\n",
    "        \n",
    "        w, h = camera_dict[\"width\"], camera_dict[\"height\"]\n",
    "        params = camera_dict[\"params\"]\n",
    "        f, cx, cy, _ = params\n",
    "        \n",
    "        pts2D = np.concatenate([pts2D, np.ones(pts2D.shape[0])[:, np.newaxis]], axis=1)\n",
    "        pts2D[:, 0] -= cx\n",
    "        pts2D[:, 1] -= cy\n",
    "        pts2D[:2] /= f\n",
    "        pts2D /= np.linalg.norm(pts2D)\n",
    "        \n",
    "        solution, sol_err = None, float(\"+inf\")\n",
    "        \n",
    "        iterator = range(self.num_models_to_eval)\n",
    "        \n",
    "        for i in (tqdm(iterator) if self.verbose else iterator):\n",
    "            try:\n",
    "                # +1 for evaluation in here\n",
    "                pts2d, idcs = self.sampler(pts2D, self.solver.get_sample_size() + 1)\n",
    "                pts3d = pts3D[idcs]\n",
    "\n",
    "                solv_res = self.solver(pts2d[:pts2d.shape[0] - 1], pts3d[:pts3d.shape[0] - 1])\n",
    "                best_sol, err = None, float(\"+inf\")\n",
    "                for sol in solv_res:\n",
    "                    R, t = sol\n",
    "                    R, t = R.detach().cpu().numpy(), t.detach().cpu().numpy()\n",
    "                    translated = R.T @ (pts3d[pts3d.shape[0] - 1] - t)\n",
    "                    translated[:2] /= translated[2]\n",
    "                    translated[:2] *= f\n",
    "                    translated[0] += cx\n",
    "                    translated[1] += cy\n",
    "                    if np.linalg.norm(translated - pts2d[pts2d.shape[0] - 1]) < err:\n",
    "                        err = np.linalg.norm(translated - pts2D[i])\n",
    "                        best_sol = sol\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            if err < sol_err:\n",
    "                sol_err = err\n",
    "                solution = best_sol \n",
    "\n",
    "        pose = poselib.CameraPose()\n",
    "        pose.q = matrix_to_quaternion(solution[0])\n",
    "        pose.t = solution[1]\n",
    "        \n",
    "        return pose\n",
    "        \n",
    "solv_pipe = UP2PSolverPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9a4a451-7a08-4cb5-972c-0f4ef2d763d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([q: 0.0916511         0  0.995791         0, t:   11.9355   -14.397 -0.907716],\n",
       " poselib.CameraPose)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = solv_pipe(\n",
    "    np.array(pts2D),\n",
    "    np.array(pts3D),\n",
    "    camera\n",
    ")\n",
    "res, type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5247103-2b96-4c94-bfea-7064c25d7ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[q:  0.770395 -0.620412  0.145098 -0.022967, t:   12.4091   4.84072 -0.453523]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose = p3pwrapper(\n",
    "    np.array(pts2D),\n",
    "    np.array(pts3D),\n",
    "    camera\n",
    ")\n",
    "pose.t = - pose.R.T @ pose.t\n",
    "pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78988031-6139-4f07-8ae7-a44b4b1f47ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(executor: SP, path: str, conf, camera_dict, gts):\n",
    "    data = np.load(path)\n",
    "    pts2D = list(data[:, :2])\n",
    "    pts3D = list(data[:, 2:])\n",
    "    pp = \"/\".join(path.split(\"/\")[-2:])\n",
    "    pp = pp.replace(\"_matches\", \"\")\n",
    "    pp = pp.replace(\".npy\", \".png\")\n",
    "    camera_dict = camera_dict[pp]\n",
    "    gt = gts[pp]\n",
    "    c, r = gt[:3], gt[3:]\n",
    "\n",
    "    pose = executor(np.array(pts2D), np.array(pts3D), camera_dict)\n",
    "\n",
    "    pose.t = - pose.R.T @ pose.t          \n",
    "\n",
    "    gt_pose = poselib.CameraPose()\n",
    "    gt_pose.q = r / np.linalg.norm(r)\n",
    "\n",
    "    rot_error = np.arccos((np.trace(np.matmul(gt_pose.R.transpose(), pose.R)) - 1.0) / 2.0) * 180.0 / np.pi\n",
    "\n",
    "    if SHOW_SINGLE:\n",
    "        print(np.trace(np.matmul(gt_pose.R.transpose(), pose.R)))\n",
    "        print(\" Position error: \" + str(np.linalg.norm(c - pose.t)) + \" orientation error: \" + str(rot_error))\n",
    "    if np.isnan(rot_error):\n",
    "        return 1000000.0, 180.0\n",
    "    else:\n",
    "        return np.linalg.norm(c - pose.t), rot_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23ecb0a8-f54e-41c2-9972-e3eb0bb2d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_camera_dict(path: str, args):\n",
    "    with open(path) as file:\n",
    "        data = file.readlines()\n",
    "\n",
    "    camera_dict = {}\n",
    "    for _, line in enumerate(data):\n",
    "        # image width, image height, focal length, x of pp, y of pp, radial distortion factor \n",
    "        path, cam_type, w, h, f, x, y, rd = line.split()\n",
    "        scaling_factor = 320 / max(np.float32(w), np.float32(h))\n",
    "  \n",
    "        # camera = {'model': 'SIMPLE_PINHOLE', 'width': 1200, 'height': 800, 'params': [960, 600, 400]}\n",
    "        camera_dict[path] = {\n",
    "          'model': cam_type,\n",
    "          'width': int(np.float32(w) * scaling_factor),\n",
    "          'height': int(np.float32(h) * scaling_factor),\n",
    "          'params': list(map(float, [np.float32(f) * scaling_factor,\n",
    "                                 np.float32(x) * scaling_factor,\n",
    "                                 np.float32(y) * scaling_factor,\n",
    "                                 np.float32(rd)])),\n",
    "        }\n",
    "  \n",
    "    return camera_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32718520-eed5-4863-b903-72f540a70216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_gts(path: str):\n",
    "    # ImageFile, Camera Position [X Y Z W P Q R]\n",
    "\n",
    "    with open(path) as file:\n",
    "        data = file.readlines()\n",
    "\n",
    "    gts = {}\n",
    "    for _, line in enumerate(data):\n",
    "        try:\n",
    "            # seq13/frame00158.png 25.317314 -0.228082 54.493720 0.374564 0.002123 0.915022 -0.149782\n",
    "            path, x, y, z, w, p, q, r = line.split()\n",
    "            rest = [x, y, z, w, p, q, r]\n",
    "            rest = list(map(float, rest))\n",
    "        except Exception as ex:\n",
    "            # print(ex)\n",
    "            continue\n",
    "        gts[path] = rest\n",
    "\n",
    "    return gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7ccff08-7bad-4ea0-9545-0e084f2f8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_dict = prepare_camera_dict(\n",
    "    \"dataset/StMarysChurch_matches/st_marys_church_list_queries_with_intrinsics_simple_radial_sorted.txt\",\n",
    "    conf\n",
    ")\n",
    "\n",
    "gt_dict = prepare_gts(\n",
    "    \"dataset/StMarysChurch_matches/dataset_test.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef86a24f-de8f-4ee5-a488-81adb5010163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:00<00:00, 219.86it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 194.15it/s]\n",
      "100%|██████████| 351/351 [00:02<00:00, 148.83it/s]\n"
     ]
    }
   ],
   "source": [
    "orientation_errors, pose_errors = [], []\n",
    "for s in seq:\n",
    "    p = f\"{VSP}/seq{s}\"\n",
    "    for f in tqdm(os.listdir(p)):\n",
    "        if f.split(\".\")[1] != \"npy\":\n",
    "            continue\n",
    "        pe, oe = process_file(p3pwrapper, f\"{p}/{f}\", conf, camera_dict, gt_dict)\n",
    "        pose_errors.append(pe)\n",
    "        orientation_errors.append(oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b84dc38-935e-4c45-aeda-20787d5df81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Couldn't localize 0 out of 530 images\n",
      " Median position error: 0.086, median orientation errors: 0.29\n",
      " Percentage of poses within the median: 41.132075471698116 % \n"
     ]
    }
   ],
   "source": [
    "pos_errors = pose_errors\n",
    "orient_errors = orientation_errors\n",
    "print(\" Couldn't localize \" + str(orientation_errors.count(180.0)) + \" out of \" + str(len(orientation_errors)) + \" images\") \n",
    "print(\" Median position error: \" +  str(round(statistics.median(pos_errors),3)) + \", median orientation errors: \" + str(round(statistics.median(orient_errors),2)))\n",
    "\n",
    "med_pos = statistics.median(pos_errors)\n",
    "med_orient = statistics.median(orient_errors)\n",
    "counter = 0\n",
    "for i in range(0, len(pose_errors)):\n",
    "    if pose_errors[i] <= med_pos and orientation_errors[i] <= med_orient:\n",
    "        counter += 1\n",
    "print(\" Percentage of poses within the median: \" + str(100.0 * float(counter) / float(len(pose_errors))) + \" % \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3283ef61-c62e-4b85-8437-558c5861c2f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:11<00:00,  8.99it/s]\n",
      "100%|██████████| 83/83 [00:07<00:00, 11.42it/s]\n",
      "100%|██████████| 351/351 [00:36<00:00,  9.62it/s]\n"
     ]
    }
   ],
   "source": [
    "orientation_errors, pose_errors = [], []\n",
    "for s in seq:\n",
    "    p = f\"{VSP}/seq{s}\"\n",
    "    for f in tqdm(os.listdir(p)):\n",
    "        if f.split(\".\")[1] != \"npy\":\n",
    "            continue\n",
    "        pe, oe = process_file(solv_pipe, f\"{p}/{f}\", conf, camera_dict, gt_dict)\n",
    "        pose_errors.append(pe)\n",
    "        orientation_errors.append(oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c27f9d12-dab9-42d3-9807-0333df328093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Couldn't localize 0 out of 530 images\n",
      " Median position error: 31.759, median orientation errors: 97.03\n",
      " Percentage of poses within the median: 24.528301886792452 % \n"
     ]
    }
   ],
   "source": [
    "pos_errors = pose_errors\n",
    "orient_errors = orientation_errors\n",
    "print(\" Couldn't localize \" + str(orientation_errors.count(180.0)) + \" out of \" + str(len(orientation_errors)) + \" images\") \n",
    "print(\" Median position error: \" +  str(round(statistics.median(pos_errors),3)) + \", median orientation errors: \" + str(round(statistics.median(orient_errors),2)))\n",
    "\n",
    "med_pos = statistics.median(pos_errors)\n",
    "med_orient = statistics.median(orient_errors)\n",
    "counter = 0\n",
    "for i in range(0, len(pose_errors)):\n",
    "    if pose_errors[i] <= med_pos and orientation_errors[i] <= med_orient:\n",
    "        counter += 1\n",
    "print(\" Percentage of poses within the median: \" + str(100.0 * float(counter) / float(len(pose_errors))) + \" % \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ad2919d-9768-4c0c-8fbf-e5518e692df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:00<00:00, 663.99it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 452.56it/s]\n"
     ]
    }
   ],
   "source": [
    "orientation_errors, pose_errors = [], []\n",
    "for s in seq[:-1]:\n",
    "    p = f\"{VSP}/seq{s}\"\n",
    "    for f in tqdm(os.listdir(p)):\n",
    "        if f.split(\".\")[1] != \"npy\":\n",
    "            continue\n",
    "        try:\n",
    "            pe, oe = process_file(ref, f\"{p}/{f}\", conf, camera_dict, gt_dict)\n",
    "            pose_errors.append(pe)\n",
    "            orientation_errors.append(oe)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "185a3729-d50c-4908-843b-509c57aa5f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Couldn't localize 0 out of 169 images\n",
      " Median position error: 27.328, median orientation errors: 129.93\n",
      " Percentage of poses within the median: 20.118343195266274 % \n"
     ]
    }
   ],
   "source": [
    "pos_errors = pose_errors\n",
    "orient_errors = orientation_errors\n",
    "print(\" Couldn't localize \" + str(orientation_errors.count(180.0)) + \" out of \" + str(len(orientation_errors)) + \" images\") \n",
    "print(\" Median position error: \" +  str(round(statistics.median(pos_errors),3)) + \", median orientation errors: \" + str(round(statistics.median(orient_errors),2)))\n",
    "\n",
    "med_pos = statistics.median(pos_errors)\n",
    "med_orient = statistics.median(orient_errors)\n",
    "counter = 0\n",
    "for i in range(0, len(pose_errors)):\n",
    "    if pose_errors[i] <= med_pos and orientation_errors[i] <= med_orient:\n",
    "        counter += 1\n",
    "print(\" Percentage of poses within the median: \" + str(100.0 * float(counter) / float(len(pose_errors))) + \" % \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cd16d0-d103-4aa3-b0d5-930813d96de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
