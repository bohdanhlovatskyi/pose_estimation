{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d60a3199-7954-4648-bb35-0b5dbdb57d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import statistics\n",
    "import numpy as np\n",
    "\n",
    "import poselib\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils.rotation_utils import get_random_upward\n",
    "from double_sol import SolverPipeline, DisplacementRefinerSolver\n",
    "from SolverPipeline import P3PBindingWrapperPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f29d9cc7-1910-483f-aecb-5f845a4d18b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78a0afcd-21da-4e57-84b0-41afdc7ba151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(max_depth=1.1, img_width=640, img_height=640, focal_length=554.2562584220409, min_depth=1.0, inliers_ratio=1.0, outlier_dist=30.0, pixel_noise=0.0)\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    max_depth: float = 10.\n",
    "    img_width: int = 640\n",
    "    img_height: int = 640\n",
    "    focal_length: int = 3 * (img_width * 0.5) / np.tan(60.0 * np.pi / 180.0);\n",
    "    min_depth: float = 1.\n",
    "    max_depth: float = 1.1\n",
    "    inliers_ratio: float = 1.\n",
    "    outlier_dist: float = 30.\n",
    "    \n",
    "    # [TODO][IMPORTNAT]: not properly tested, be aware of using for\n",
    "    # some experiments\n",
    "    pixel_noise: float = 0.\n",
    "    \n",
    "conf = Config()\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b8bf7d-2b0e-4d37-aa58-64ac79fe7ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "133552b5-8ba1-4167-aea0-d487807319f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = DisplacementRefinerSolver(verbose=False)            \n",
    "p3p_solv_pipe = SolverPipeline()\n",
    "p2p_solv_pipe = SolverPipeline(up2p=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ca80f11d-2b71-42de-b458-fc796debf0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_image_point(conf: Config):\n",
    "    x = random.uniform(0, conf.img_width)\n",
    "    y = random.uniform(0, conf.img_height)\n",
    "    x = torch.tensor([x, y], dtype=torch.float64)    \n",
    "    return x\n",
    "\n",
    "def to_homogeneous(x):\n",
    "    return torch.cat([x, torch.ones(1)])\n",
    "\n",
    "def to_camera_coords(x: torch.tensor, conf: Config = conf):\n",
    "    x = to_homogeneous(x)\n",
    "    \n",
    "    x[0] -= conf.img_width // 2\n",
    "    x[1] -= conf.img_height // 2\n",
    "    x[:2] /= conf.focal_length\n",
    "    x /= x.norm()\n",
    "    \n",
    "    return x\n",
    "\n",
    "def generate_correspondence(x: torch.tensor, conf: Config):\n",
    "    x = to_camera_coords(x, conf)\n",
    "    x *= random.uniform(conf.min_depth, conf.max_depth)\n",
    "    \n",
    "    assert x.shape == (3,)    \n",
    "    return x\n",
    "\n",
    "def transform_correspondence(X: torch.tensor, R: torch.tensor, t: torch.tensor):\n",
    "    return R @ X + t\n",
    "\n",
    "def generate_example(R, t, conf: Config = conf):\n",
    "    x1, x2 = get_random_image_point(conf), get_random_image_point(conf)\n",
    "    X1, X2 = generate_correspondence(x1.clone(), conf),\\\n",
    "             generate_correspondence(x2.clone(), conf)\n",
    "    X1, X2 = transform_correspondence(X1, R, t), transform_correspondence(X2, R, t)\n",
    "    \n",
    "    # [TODO][IMPORTNAT]: not properly tested, be aware of using for\n",
    "    # some experiments\n",
    "    if conf.pixel_noise != 0:\n",
    "        x1noise = np.random.normal(0, conf.pixel_noise, 2)\n",
    "        x2noise = np.random.normal(0, conf.pixel_noise, 2)\n",
    "        \n",
    "        if torch.all(x1[0] + x1noise > 0) and torch.all(x1[1] + x1noise < conf.img_width):\n",
    "            x1 += x1noise\n",
    "            assert x1[0] > 0 and x1[1] < conf.img_width, f\"{x1}\"\n",
    "            \n",
    "        if torch.all(x2[0] + x2noise > 0) and torch.all(x2[1] + x2noise < conf.img_height):\n",
    "            x2 += x2noise\n",
    "            assert x2[0] > 0 and x2[1] < conf.img_height, f\"{x2}\"\n",
    "        \n",
    "        assert x1[0] > 0 and x1[1] < conf.img_width, f\"{x1}\"\n",
    "        assert x2[0] > 0 and x2[1] < conf.img_height, f\"{x2}\"\n",
    "\n",
    "    return x1, x2, X1, X2 \n",
    "\n",
    "# x: [2, ]\n",
    "def generate_outlier(x, conf):\n",
    "    out = get_random_image_point(conf)\n",
    "    \n",
    "    while ((out - x).norm() < conf.outlier_dist):\n",
    "        out = get_random_image_point(conf)\n",
    "        \n",
    "    return out\n",
    "\n",
    "from typing import Tuple\n",
    "def generate_examples(num_of_examples: int,\n",
    "                      dev: Tuple[float, float] = (0., 0.), conf: Config = conf):\n",
    "    num_of_examples = num_of_examples // 2\n",
    "    \n",
    "    num_inliers = num_of_examples * conf.inliers_ratio\n",
    "    num_outliers = num_of_examples - num_inliers\n",
    "    \n",
    "    if num_of_examples == 0:\n",
    "        num_of_examples, num_inliers, num_outliers = 1, 1, 0\n",
    "    \n",
    "    R, rand_angle = get_random_upward(*dev)\n",
    "    t = torch.rand(3, )\n",
    "        \n",
    "    # [TODO] [IMPORTANT]: under such generation we cannot get model where one of the points is an inlier\n",
    "    xs, Xs, inliers = [], [], []\n",
    "    for i in range(num_of_examples):\n",
    "        x1, x2, X1, X2 = generate_example(R, t)\n",
    "        Xs.append((X1, X2))\n",
    "        \n",
    "        if i < num_inliers:\n",
    "            xs.append((x1, x2))\n",
    "            inliers.append(True)\n",
    "        else:\n",
    "            xs.append((generate_outlier(x1, conf), generate_outlier(x2, conf)))\n",
    "            inliers.append(False)\n",
    "            \n",
    "    xs = np.concatenate([[p.numpy() for p in elm] for elm in xs])\n",
    "    Xs = np.concatenate([[p.numpy() for p in elm] for elm in Xs])\n",
    "    \n",
    "            \n",
    "    return xs, Xs, inliers, R, t, rand_angle\n",
    "\n",
    "def compute_metric(Rgt, tgt, R, t):\n",
    "    if R is None or t is None:\n",
    "        return 1000000.0, 180.0\n",
    "      \n",
    "    rot_error = np.arccos((np.trace(np.matmul(R.T, Rgt)) - 1.0) / 2.0) * 180.0 / np.pi\n",
    "    if np.isnan(rot_error):\n",
    "        return 1000000.0, 180.0\n",
    "    else:\n",
    "        return np.linalg.norm(tgt - t), rot_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "202b98c8-3ee6-45de-b901-619b71b0040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = DisplacementRefinerSolver(verbose=False)            \n",
    "p3p_solv_pipe = SolverPipeline()\n",
    "p2p_solv_pipe = SolverPipeline(up2p=True)\n",
    "p3pwrapper = P3PBindingWrapperPipeline(\n",
    "        ransac_conf = {\n",
    "            # 'max_reproj_error': args.ransac_thresh\n",
    "            'min_iterations': 100,\n",
    "            'max_iterations': 10000,\n",
    "            'progressive_sampling': True,\n",
    "            'max_prosac_iterations': 13\n",
    "            },\n",
    "            \n",
    "            bundle_adj_conf = {\n",
    "                'loss_scale' : 1.0,\n",
    "            }                                              \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "386f554b-2b93-42e0-adee-97960a453cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.80s/it]\n"
     ]
    }
   ],
   "source": [
    "camera_dict = {\n",
    "    \"width\": conf.img_width, \n",
    "    \"height\": conf.img_height, \n",
    "    \"params\": [conf.focal_length, conf.img_width // 2, conf.img_height // 2, 0]\n",
    "}\n",
    "\n",
    "orientation_errors_p3pr, pose_errors_p3pr = [], []\n",
    "orientation_errors_p3p, pose_errors_p3p = [], []\n",
    "orientation_errors_np, pose_errors_np = [], []\n",
    "orientation_errors_p, pose_errors_p = [], []\n",
    "    \n",
    "seed = 13\n",
    "for _ in tqdm(range(3)):\n",
    "    xs, Xs, _, Rgt, tgt, rand_angle = generate_examples(10, (0, 0), conf)\n",
    "    \n",
    "    # np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # R, t = p3pwrapper(xs, Xs, camera_dict)\n",
    "    # pose_error_p3pr, orient_error_p3pr = dataset.compute_metric(Rgt, tgt, R, t)\n",
    "    # # print(\"rp3p[pe, oe]: \", pose_error_p3pr, orient_error_p3pr, Rotation.from_matrix(R).as_euler(\"XYZ\", degrees=True), t)\n",
    "    # orientation_errors_p3pr.append(orient_error_p3pr)\n",
    "    # pose_errors_p3pr.append(pose_error_p3pr)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    R, t = p3p_solv_pipe(xs, Xs, camera_dict) \n",
    "    pose_error_p3p, orient_error_p3p = compute_metric(Rgt, tgt, R.numpy(), t.numpy())\n",
    "    # print(\"p3p[pe, oe]: \", pose_error_p3p, orient_error_p3p, Rotation.from_matrix(R).as_euler(\"XYZ\", degrees=True), t.numpy())\n",
    "    orientation_errors_p3p.append(orient_error_p3p)\n",
    "    pose_errors_p3p.append(pose_error_p3p)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    R, t = p2p_solv_pipe(xs, Xs, camera_dict) \n",
    "    pose_error_np, orient_error_np = compute_metric(Rgt, tgt, R.numpy(), t.numpy())\n",
    "    # print(\"np[pe, oe]: \", pose_error_np, orient_error_np, Rotation.from_matrix(R).as_euler(\"XYZ\", degrees=True), t.numpy())\n",
    "    orientation_errors_np.append(orient_error_np)\n",
    "    pose_errors_np.append(pose_error_np)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    R, t = ref(xs, Xs, camera_dict)\n",
    "    pose_error_p, orient_error_p = compute_metric(Rgt, tgt, R, t)\n",
    "    # print(\"p[pe, oe]: \", pose_error_p, orient_error_p, Rotation.from_matrix(R).as_euler(\"XYZ\", degrees=True), t)\n",
    "    orientation_errors_p.append(orient_error_p)\n",
    "    pose_errors_p.append(pose_error_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "800fb53b-1f15-42ef-bb6e-c0be33d31a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(pose_errors, orientation_errors):\n",
    "    pos_errors = pose_errors\n",
    "    orient_errors = orientation_errors\n",
    "    print(\" Couldn't localize \" + str(orientation_errors.count(180.0)) + \" out of \" + str(len(orientation_errors)) + \" images\") \n",
    "    print(\" Median position error: \" +  str(round(statistics.median(pos_errors),3)) + \", median orientation errors: \" + str(round(statistics.median(orient_errors),2)))\n",
    "\n",
    "    med_pos = statistics.median(pos_errors)\n",
    "    med_orient = statistics.median(orient_errors)\n",
    "    counter = 0\n",
    "    for i in range(0, len(pose_errors)):\n",
    "        if pose_errors[i] <= med_pos and orientation_errors[i] <= med_orient:\n",
    "            counter += 1\n",
    "    print(\" Percentage of poses within the median: \" + str(100.0 * float(counter) / float(len(pose_errors))) + \" % \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4a27508f-7542-473e-a249-b89f91e29827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Couldn't localize 0 out of 3 images\n",
      " Median position error: 0.955, median orientation errors: 101.05\n",
      " Percentage of poses within the median: 66.66666666666667 % \n",
      " Couldn't localize 0 out of 3 images\n",
      " Median position error: 0.689, median orientation errors: 116.92\n",
      " Percentage of poses within the median: 33.333333333333336 % \n",
      " Couldn't localize 0 out of 3 images\n",
      " Median position error: 2.077, median orientation errors: 137.64\n",
      " Percentage of poses within the median: 33.333333333333336 % \n"
     ]
    }
   ],
   "source": [
    "print_stats(pose_errors_p3p, orientation_errors_p3p)\n",
    "print_stats(pose_errors_np, orientation_errors_np)\n",
    "print_stats(pose_errors_p, orientation_errors_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc49a8-fa13-42cb-bcd6-a6da83071156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
